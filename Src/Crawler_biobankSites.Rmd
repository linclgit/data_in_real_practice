---
output: html_document
knit: (function(inputFile, encoding) {
  rmarkdown::render(inputFile, encoding = encoding, output_dir = "../output/") })
---

```{r, echo = FALSE}
knitr::opts_chunk$set (collapse=TRUE, comment="##", fig.path = "../images/", warning = FALSE)
```

```{r message=FALSE, warning=FALSE}
library (rvest); library(httr)
library (tidyverse); library(ggplot2)
library (maps); library(mapdata); library(ggmap)
library (RColorBrewer)
```

### Example webpage: this page shows the contact information of several work sites of the organization
```{r}
url = "https://www.twbiobank.org.tw/new_web/contact.php"
res = GET (url) # use GET method to get the web response
res
```

### A quick view of the web response
```{r}
# str (res)  
# names (res)
# res$all_headers
# res$request
# res$request$headers  
# res$cookies  
# res$content
```


### Info we want is usually in "content"
```{r}
# content(res, as = "raw") # binary codes
# content(res, as = "text") # default encoding method is utf-8
```

### 1-- Scrape the work site addresses
```{r}
address <- res %>% 
  content (as = "text") %>% 
  read_html %>% 
  html_nodes (xpath = "//div[@class='panel address']") %>% 
  html_text %>% # turn the content into text
  as.data.frame # turn the table into data.frame

#class (address)
#str(address)
head (address) # Overview of the scraped content
```

### Some data cleaning:    
divide the location string into the following columns, city, site, address, and building    
    
```{r}
address_df <- address %>% 
  separate (".", into = c ("city", "location"), sep = 2) %>% 
  slice (2:n()) # select 2nd to n rows

for (i in 1:nrow(address_df))
{
  address_df$location[i] <- 
    ifelse (grepl("駐站", address_df$location[i]) == FALSE, 
            sub ("醫院", "醫院駐站", address_df$location[i]), address_df$location[i])
}

address_df <- address_df %>% 
  mutate (site = sub ("駐站.*", "", address_df$location),
          address = sub ("^.*駐站", "", address_df$location)) %>% 
  separate ("address", into = c ("address", "building"), sep = " ") %>% 
  select (-location) %>% 
  mutate (type = "cohort") 

address_df$type[32:38] <- "medical_center"
  
address_df$address[31] <- sub (".*)", "", address_df$address[31])

head (address_df)
```

### 2-- Scrape the coordinate info
```{r}
geo <- res %>% 
  content (as = "text") %>% 
  read_html %>% 
  html_nodes (xpath = "//li[@data-geo-lat]") %>% 
  html_attrs 
```

### Some data cleaning:    
turn the list into data frame and rename columns    
    
```{r}
geo_df <- t (as.data.frame (geo, col.names = paste0("X", c(1:34)))) 
geo_df <- as.data.frame(geo_df)
head (geo_df)

colnames (geo_df) <- c("lat", "long")
row.names (geo_df) <- NULL
```

### 3-- Combine the 2 tables
```{r}
stations <- cbind (address_df, geo_df)
library (knitr); library (kableExtra)
kable (stations, format = "html") %>% 
  kable_styling(bootstrap_options = "striped", font_size = 12) %>%
  scroll_box(height = "300px")
```

### 4-- Plot the sites    
 * mose over to see site names   
    
```{r}
stations$lat <- as.numeric(as.character(stations$lat))
stations$long <- as.numeric(as.character(stations$long))
#tw <- map_data("world2")[which(map_data("world2")$region == "Taiwan"),]

library(leaflet)
leaflet() %>% addProviderTiles(providers$Stamen.TonerLite) %>% 
  setView (lng = 121, lat = 23.5, zoom = 7) %>% 
  addCircles(data = stations %>% filter (type=="cohort"), color = "red", 
             lng = ~long, lat = ~lat, weight = 5, label = ~site) %>%
  addCircles(data = stations %>% filter (type=="medical_center"), color = "blue",
             lng = ~long, lat = ~lat, weight = 5, label = ~site)
```

